Index: LicentaPeBune/agent.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/LicentaPeBune/agent.py b/LicentaPeBune/agent.py
--- a/LicentaPeBune/agent.py	
+++ b/LicentaPeBune/agent.py	
@@ -1,113 +1,94 @@
-import torch
-import random
+import gym
 import numpy as np
 from collections import deque
-from training import Linear_QNet, QTrainer
+import matplotlib.pyplot as plt
+plt.rcParams['figure.figsize'] = (16, 10)
 from AIplayed import Game
-from plothelper import plot
-from Gameproperties import Properties
-import pygame
-
-MAX_MEMORY = 1000_000
-BATCH_SIZE = 1000
-LR = 0.0005
-
-class Agent:
-
-    def __init__(self):
-        Properties.n_games = 0
-        self.epsilon = 2000 # randomness
-        self.gamma = 0.98 # discount rate
-        self.memory = deque(maxlen=MAX_MEMORY) # popleft()
-        self.model = Linear_QNet(1, 2, 3)
-        self.trainer = QTrainer(self.model, lr=LR, gamma=self.gamma)
-
-    def metoda(self, PT1 , Blocks0 ):
-
-        if (PT1.pos.x - Blocks0.pos.x) == 0:
-            return 0
-        if (PT1.pos.y - Blocks0.pos.y) == 0:
-            if (PT1.pos.x - Blocks0.pos.x) > 0:
-                return 2
-            return -2
-        return np.arctan((PT1.pos.x - Blocks0.pos.x) / (PT1.pos.y - Blocks0.pos.y))
-    def get_state(self, game):
-        state = [
-            self.metoda(game.PT1, game.Blocks0)
-        ]
-        return state
-
-    def remember(self, state, action, reward, next_state, done):
-        self.memory.append((state, action, reward, next_state, done)) # popleft if MAX_MEMORY is reached
-
-    def train_long_memory(self):
-        if len(self.memory) > BATCH_SIZE:
-            mini_sample = random.sample(self.memory, BATCH_SIZE)
-        else:
-            return
-        states, actions, rewards, next_states, dones = zip(*mini_sample)
-        self.trainer.train_step(states, actions, rewards, next_states, dones)
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+from torch.distributions import Categorical
+torch.manual_seed(0)
 
-    def train_short_memory(self, state, action, reward, next_state, done):
-        self.trainer.train_step(state, action, reward, next_state, done)
 
-    def get_action(self, state):
-        # Epsilon-greedy strategy
 
-        if random.randint(0, 200) < self.epsilon:
-            # Explore: select a random action
-            final_move = random.choice([0, 1, 2])
-        else:
-            # Exploit: select the action with max value (greedy)
-            state0 = torch.tensor(state, dtype=torch.float)
-            prediction = self.model(state0)
-            final_move = torch.argmax(prediction).item()
-        self.epsilon = 80 * 0.99 ** Properties.n_games
-        return final_move
-
-    def train(self):
-        plot_scores = []
-        plot_mean_scores = []
-        total_score = 0
-        game = Game()
-        game.loadgrafic()
-        game.blocksgroup.add(game.Blocks0)
-        game.all_sprites.add(game.PT1)
-        game.all_sprites.add(game.Blocks0)
+game = Game()
+env = gym.make('CartPole-v0')
+env.seed(0)
+
+print('observation space:', env.observation_space)
+print('action space:', env.action_space)
+
+class Policy(nn.Module):
+    def __init__(self, state_size=1, action_size=3, hidden_size=32):
+        super(Policy, self).__init__()
+        self.fc1 = nn.Linear(state_size, hidden_size)
+        self.fc2 = nn.Linear(hidden_size, action_size)
+
+    def forward(self, state):
+        x = F.relu(self.fc1(state))
+        x = self.fc2(x)
+        # we just consider 1 dimensional probability of action
+        return F.softmax(x, dim=1)
+
+    def act(self, state):
+        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
+        probs = self.forward(state).cpu()
+        model = Categorical(probs)
+        action = model.sample()
+        return action.item(), model.log_prob(action)
 
-        while Properties.running:
-            for event in pygame.event.get():
-                if event.type == pygame.QUIT:
-                    Properties.running = False
-            if Properties.running == False:
+
+def reinforce(policy, optimizer, n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):
+
+    scores_deque = deque(maxlen=100)
+    scores = []
+    for e in range(1, n_episodes):
+        saved_log_probs = []
+        rewards = []
+        state = env.reset()
+        # Collect trajectory
+        for t in range(max_t):
+            # Sample the action from current policy
+            action, log_prob = policy.act(state)
+            saved_log_probs.append(log_prob)
+            state, reward, done, _ = game.play_step(final_move)
+            rewards.append(reward)
+            if done:
                 break
-            state_old = self.get_state(game)
-            final_move = self.get_action(state_old)
-            reward, done, score = game.play_step(final_move)
-            state_new = self.get_state(game)
-            self.train_short_memory(state_old, final_move, reward, state_new, done)
-            self.remember(state_old, final_move, reward, state_new, done)
+        # Calculate total expected reward
+        scores_deque.append(sum(rewards))
+        scores.append(sum(rewards))
 
-            if Properties.running == False:
-                Properties.running=True
-                game.reset()
-                Properties.n_games += 1
-                self.train_long_memory()
+        # Recalculate the total reward applying discounted factor
+        discounts = [gamma ** i for i in range(len(rewards) + 1)]
+        R = sum([a * b for a, b in zip(discounts, rewards)])
 
-                if score > Properties.maxscore:
-                    Properties.maxscore = score
-                    self.model.save()
-                    print("saved")
+        # Calculate the loss
+        policy_loss = []
+        for log_prob in saved_log_probs:
+            # Note that we are using Gradient Ascent, not Descent. So we need to calculate it with negative rewards.
+            policy_loss.append(-log_prob * R)
+        # After that, we concatenate whole policy loss in 0th dimension
+        policy_loss = torch.cat(policy_loss).sum()
 
-                plot_scores.append(score)
-                total_score += score
-                mean_score = total_score / Properties.n_games
-                plot_mean_scores.append(mean_score)
-                plot(plot_scores, plot_mean_scores)
+        # Backpropagation
+        optimizer.zero_grad()
+        policy_loss.backward()
+        optimizer.step()
 
-                print('Game', Properties.n_games, 'Score', score, 'Record:', Properties.maxscore)
+        if e % print_every == 0:
+            print('Episode {}\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))
+        if np.mean(scores_deque) >= 195.0:
+            print('Environment solved in {:d} episodes!\tAverage Score: {:.2f}'.format(e - 100, np.mean(scores_deque)))
+            break
+    return scores
 
-if __name__ == "__main__":
 
-    agent = Agent()
-    agent.train()
\ No newline at end of file
+    fig = plt.figure()
+    ax = fig.add_subplot(111)
+    plt.plot(np.arange(1, len(scores)+1), scores)
+    plt.ylabel('Score')
+    plt.xlabel('Episode #')
+    plt.show()
\ No newline at end of file
